{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPyZDGFGBXkE"
      },
      "source": [
        "<span style=\"font-size:3em; text-align:center\">Information System Security -</span>\n",
        "\n",
        "<span style=\"font-size:3em; text-align:center\">Anomaly-based Intrusion Detection System</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3uRyOJ2B12M",
        "outputId": "edd0b35f-4537-4e34-81b9-c49817e61850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C65VsLRzBXkI"
      },
      "source": [
        "Data is contained in 8 different CSV files, each containing different attack data at different times. So first thing we must do is merge all the data from files into one pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz5WzNaWBXkJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA2sJWfkBXkL"
      },
      "outputs": [],
      "source": [
        "# Saving all .csv files in folder to list.\n",
        "path = \"/content/drive/MyDrive/CSVs\"\n",
        "files = [file for file in glob.glob(path + \"**/*.csv\", recursive=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s21dS9YABXkM",
        "outputId": "7f3389ed-f829-4123-deb7-0a6456405553",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CSVs/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Monday-WorkingHours.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Wednesday-workingHours.pcap_ISCX.csv\n",
            "/content/drive/MyDrive/CSVs/Copy of Monday-WorkingHours.pcap_ISCX.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "[print(f) for f in files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIR5PVXKBXkN",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f690188c-4407-4c4a-fb70-ee415f26496e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-6fd501f313bf>:3: DtypeWarning: Columns (0,1,3,6,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataset = [pd.read_csv(f,  encoding='latin1') for f in files]\n",
            "<ipython-input-20-6fd501f313bf>:3: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataset = [pd.read_csv(f,  encoding='latin1') for f in files]\n"
          ]
        }
      ],
      "source": [
        "# Reading all the csv files into dataframes and putting thoose DFs to one list.\n",
        "\n",
        "dataset = [pd.read_csv(f,  encoding='latin1') for f in files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al0SiJyxBXkO",
        "outputId": "d445227e-c698-40a1-c546-c592a5e262c1",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(191033, 85)\n",
            "(288602, 85)\n",
            "(458968, 85)\n",
            "(286467, 85)\n",
            "(225745, 86)\n",
            "(445909, 85)\n",
            "(529918, 85)\n",
            "(692703, 85)\n",
            "(529918, 85)\n"
          ]
        }
      ],
      "source": [
        "# Here we can see the number of rows and columns for each table.\n",
        "\n",
        "for d in dataset:\n",
        "    print(d.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SpSLuJpoBXkO",
        "outputId": "57065afa-3a5f-483d-cc52-974e8009a215"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Lengths must match to compare",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ba8ae8b89b7e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msame_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msame_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7184\u001b[0m         ) != len(other):\n\u001b[0;32m-> 7185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lengths must match to compare\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Lengths must match to compare"
          ]
        }
      ],
      "source": [
        "# We already established that all tables have the same number of columns, but are they the same columns?\n",
        "# This next piece of code loops over all given tables and compares each of them to all others.\n",
        "\n",
        "for i in range(0,len(dataset)):\n",
        "  if i != len(dataset)-1:\n",
        "    same_columns = dataset[i].columns == dataset[i+1].columns\n",
        "\n",
        "    if False in same_columns:\n",
        "      print(i)\n",
        "      break\n",
        "\n",
        "same_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTnotCaABXkP"
      },
      "outputs": [],
      "source": [
        "# Combining all tables into one dataset. This is possilbe since all tables have the same columns,\n",
        "# as we checked in the cell above.\n",
        "\n",
        "dataset = pd.concat([d for d in dataset]).drop_duplicates(keep=False)\n",
        "dataset.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pr8LVrNBXkP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# By checking the shape of dataset we can confirm that concatenation has been successfull.\n",
        "\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JF4r9U6BXkQ"
      },
      "source": [
        "# Preliminary data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnn48QlkBXkQ"
      },
      "source": [
        "Some general info about the dataset. It contains roughly 2.5 million records across 79 columns. Data consists of mostly int64 and float64 types, except 3 attributes of 'object' type.\n",
        "\n",
        "Dataset contains of network traffic data during different attacks, represented with values like: port numbers, IP adressses, packet lenghts, SYN/ACK/FIN/.. flag counts, packet size and other..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YkBDP0tnBXkQ"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv('Dataset_clean.csv', index_col=[0])\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVCEWQ4XBXkR"
      },
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty5LKtVUBXkR"
      },
      "source": [
        "Upon further inspection we can see that dataset contains 15 labels. Labels represent network/web attacks and BENIGN state which is the network traffic during normal business day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7p_nSgsBXkR",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Dataset conatains 15 labels.\n",
        "#print(dataset[' Label'].unique())\n",
        "#len(dataset[' Label'].unique())\n",
        "\n",
        "print(dataset[' Label'].unique())\n",
        "len(dataset[' Label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYgRjg92BXkR"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93kCWUbuBXkS"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgBw_S-BBXkS"
      },
      "source": [
        "Most records in the dataset are of DDos and DOS Hulk attacks. This might pose a problem later in model training, considering that there is a very small amount of data for most attacks. Model selection will be greatly influenced by this information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KpVOtQwBXkS"
      },
      "outputs": [],
      "source": [
        "data = dataset[' Label'].where(dataset[' Label'] != \"BENIGN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQVmTUp6BXkS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "chart = sns.countplot(data, palette=\"Set1\")\n",
        "plt.xticks(rotation=45, horizontalalignment=\"right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-oCYsWYBXkS"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMpX05b8BXkT"
      },
      "source": [
        "This chapter contains data cleaning code. We go through the process of renaming columns, removing NaN and non-finite values (-inf, inf) to get the data ready for visualization and model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwYz6a_BXkT"
      },
      "source": [
        "## Renaming columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmAkxMhUBXkT"
      },
      "outputs": [],
      "source": [
        "# Removing whitespaces in column names.\n",
        "\n",
        "col_names = [col.replace(' ', '') for col in dataset.columns]\n",
        "dataset.columns = col_names\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjGH6GhpBXkT"
      },
      "outputs": [],
      "source": [
        "# Here we can see that 'Label' column contains some wierd characters.\n",
        "\n",
        "dataset[\"Label\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NOT71OBBXkT"
      },
      "outputs": [],
      "source": [
        "# This next snippet uses regular expressions to replace wierd characters with dunders.\n",
        "\n",
        "label_names = dataset['Label'].unique()\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "label_names = [re.sub(\"[^a-zA-Z ]+\", \"\", l) for l in label_names]\n",
        "label_names = [re.sub(\"[\\s\\s]\", '_', l) for l in label_names]\n",
        "label_names = [lab.replace(\"__\", \"_\") for lab in label_names]\n",
        "\n",
        "label_names, len(label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEevbISeBXkT"
      },
      "outputs": [],
      "source": [
        "# Replacing 'Label' column values with new readable values.\n",
        "\n",
        "labels = dataset['Label'].unique()\n",
        "\n",
        "for i in range(0,len(label_names)):\n",
        "    dataset['Label'] = dataset['Label'].replace({labels[i] : label_names[i]})\n",
        "\n",
        "dataset['Label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UUY2BfIBXkT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "len(dataset['Label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIWvZY3oBXkT"
      },
      "outputs": [],
      "source": [
        " #Saving cleaned dataset.\n",
        "\n",
        "dataset.to_csv(\"Dataset_cleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG8MzwcgBXkU"
      },
      "source": [
        "## Removing NULL values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2S9Ku-8BXkU"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv(\"Dataset_clean.csv\", index_col=0)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6eyexmQBXkU"
      },
      "outputs": [],
      "source": [
        "# Checking if there are any NULL values in the dataset.\n",
        "\n",
        "dataset.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15A1XqVPBXkU"
      },
      "outputs": [],
      "source": [
        "# Checking which column/s contain NULL values.\n",
        "\n",
        "[col for col in dataset if dataset[col].isnull().values.any()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZWSb8xVBXkU"
      },
      "outputs": [],
      "source": [
        "# Checking how many NULL values it this column contains.\n",
        "\n",
        "dataset['FlowBytes/s'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmug81DmBXkV"
      },
      "outputs": [],
      "source": [
        "# Considering that only 334 rows contain NULL vlaues in the entire dataset, which makes about 0.01%, we\n",
        "# can safely remove all NULL rows without spoiling the data.\n",
        "\n",
        "334/dataset.shape[0]*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-aJ5jwXBXkV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Removing rows that contain NULL values and checking if number of removed rows is equal to the number of null values.\n",
        "\n",
        "before = dataset.shape\n",
        "\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "after = dataset.shape\n",
        "\n",
        "before[0] - after[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Gu1WFH_BXkV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dataset.isnull().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yTh9jQKBXkV"
      },
      "source": [
        "## Removing *non-finite* values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl9GejN0BXkV"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfctoJQvBXkV"
      },
      "outputs": [],
      "source": [
        "labl = dataset['Label']\n",
        "dataset = dataset.loc[:, dataset.columns != 'Label'].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2bOUDEgBXkV"
      },
      "outputs": [],
      "source": [
        "# Checking if all values are finite.\n",
        "\n",
        "np.all(np.isfinite(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJdyoh7SBXkW"
      },
      "outputs": [],
      "source": [
        "# Checking what column/s contain non-finite values.\n",
        "\n",
        "nonfinite = [col for col in dataset if not np.all(np.isfinite(dataset[col]))]\n",
        "\n",
        "nonfinite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l78aGXCTBXkW"
      },
      "outputs": [],
      "source": [
        "# Checking how many non-finite values each column contains.\n",
        "\n",
        "finite = np.isfinite(dataset['FlowBytes/s']).sum()\n",
        "\n",
        "dataset.shape[0] - finite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAz8WfJ8BXkW"
      },
      "outputs": [],
      "source": [
        "# Checking how many non-finite values each column contains.\n",
        "\n",
        "finite = np.isfinite(dataset['FlowPackets/s']).sum()\n",
        "\n",
        "dataset.shape[0] - finite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stuW5YMKBXkW"
      },
      "outputs": [],
      "source": [
        "# Same as before, since there is a small number of non-finite values we can safely remove them from the dataset\n",
        "# without spoiling the dataset.\n",
        "\n",
        "# Replacing infinite values with NaN values.\n",
        "dataset = dataset.replace([np.inf, -np.inf], np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjwIXmySBXkW"
      },
      "outputs": [],
      "source": [
        "# We can see that now we have Nan values again.\n",
        "\n",
        "np.any(np.isnan(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inMDgPxCBXkW"
      },
      "outputs": [],
      "source": [
        "# Bringing the Labels back into the dataset before deliting Nan rows.\n",
        "\n",
        "dataset = dataset.merge(labl, how='outer', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igFD35UuBXkW"
      },
      "outputs": [],
      "source": [
        "# Removing new NaN values.\n",
        "\n",
        "dataset.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqHQWSdqBXkW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTXKF-ZtBXkW"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSF4SD0YBXkX"
      },
      "outputs": [],
      "source": [
        "# Saving cleaned dataset.\n",
        "\n",
        "dataset.to_csv(\"Dataset_clean_dropna.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJmSJEBOBXkX"
      },
      "source": [
        "# Data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edLF4Z5CBXkX"
      },
      "source": [
        "So, by now we know our dataset has 78 features and is split into 15 categories (14 attacks and 1 \"normal\" state).\n",
        "Next step is to try and visualize what the dataset looks like in feature space.\n",
        "For this we will use principal component analysis (PCA) to reduce dimensionality and then pass the reduced dataset to t-SNE (t - Distributed Stohastic Neighbor Entities) for visual representation in 2D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxbQFUVFBXkX"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjPPaDesBXkX"
      },
      "outputs": [],
      "source": [
        "# We are going to pick 10.000 random rows from the dataset for visualization purposes.\n",
        "# Setting the random seed for reproducability of results.\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "rand_perm = np.random.permutation(dataset.shape[0])\n",
        "feature_cols = dataset.columns[:-1]\n",
        "\n",
        "dataset_subset = dataset.loc[rand_perm[:10000],:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfWoHvbnx235"
      },
      "outputs": [],
      "source": [
        "# Use iloc to select the rows based on integer-location indexing\n",
        "np.random.seed(42)\n",
        "rand_perm = np.random.permutation(dataset.shape[0])\n",
        "\n",
        "dataset_subset = dataset.iloc[rand_perm[:10000], :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDekQ06zyG_J"
      },
      "outputs": [],
      "source": [
        "# We are going to pick 10.000 random rows from the dataset for visualization purposes.\n",
        "# Setting the random seed for reproducability of results.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Instead of using permutation of the dataset shape, use the dataset index\n",
        "rand_perm = np.random.choice(dataset.index, size=10000, replace=False)\n",
        "feature_cols = dataset.columns[:-1]\n",
        "\n",
        "dataset_subset = dataset.loc[rand_perm,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSsluw1wBXkX"
      },
      "outputs": [],
      "source": [
        "dataset_subset = dataset_subset.replace([np.inf, -np.inf], np.nan)\n",
        "dataset_subset.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1AnIOt3BXkX"
      },
      "outputs": [],
      "source": [
        "data_subset = dataset_subset[feature_cols].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFwTK3k9BXkX"
      },
      "outputs": [],
      "source": [
        "# Performing the principal component analysis. With just 19 components the variance ratio remains 99%, which is great.\n",
        "\n",
        "pca = PCA(n_components=19)\n",
        "pca_res = pca.fit_transform(data_subset)\n",
        "\n",
        "data_subset = None\n",
        "np.sum(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zigyKuMMBXkY"
      },
      "outputs": [],
      "source": [
        "# Computing t-SNE.\n",
        "\n",
        "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=1000)\n",
        "tsne_res = tsne.fit_transform(data_subset)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nIS7qdd0Fg_"
      },
      "outputs": [],
      "source": [
        "# Use the PCA results for t-SNE\n",
        "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=1000)\n",
        "tsne_res = tsne.fit_transform(pca_res)\n",
        "print(\"done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSc-ZuKJBXkY"
      },
      "outputs": [],
      "source": [
        "dataset_subset['tsne_firstD'] = tsne_res[:,0]\n",
        "dataset_subset['tsne_secondD'] = tsne_res[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDKmzc2TBXkY",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,16))\n",
        "\n",
        "sns.scatterplot(\n",
        "    x=\"tsne_firstD\", y=\"tsne_secondD\",\n",
        "    palette=sns.color_palette(\"hls\", colors),\n",
        "    data=dataset_subset,\n",
        "    hue=\"Label\",\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiqGVyUL0t6m"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'dataset_subset' and 'tsne_res' are defined\n",
        "\n",
        "# Calculate the number of unique labels for the color palette\n",
        "colors = len(dataset_subset['Label'].unique())\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "sns.scatterplot(\n",
        "    x=\"tsne_firstD\", y=\"tsne_secondD\",\n",
        "    palette=sns.color_palette(\"hls\", colors), # Use the calculated 'colors' value here\n",
        "    data=dataset_subset,\n",
        "    hue=\"Label\",\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBxDvI2uBXkY"
      },
      "source": [
        "From the cell above we can see distribution of the data in 2D space. It is obvious that attacks are not spatialy well separated from normal state. Clusters of attacks can hardly be seen, instead they are found in the same place as the \"normal state\" datatpoints.\n",
        "\n",
        "This insight leads us to conclude that the ML model will probably have some issues with this kind of data. ML model will have to be chosen with this in mind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IueCD92YBXkY"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gnD4VAlBXkY"
      },
      "source": [
        "In this chapter, final data preparation steps are taken before we use the data for model traning and testing.\n",
        "\n",
        "These steps include:\n",
        "\n",
        "* Data scaling\n",
        "* Label encoding\n",
        "* Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsvn9y2_BXkY"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv(\"Dataset_clean_dropna.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Full Dataset\n",
        "# Replace 'path_to_your_csv_file.csv' with the actual path to your large dataset\n",
        "file_path = '/content/drive/MyDrive/Dataset_clean_dropna.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Loaded full dataset with shape:\", df.shape)\n",
        "\n",
        "# Step 2: Sample a Subset of the Data\n",
        "# Specify the sample fraction (e.g., 0.1 for 10% of the data)\n",
        "sample_fraction = 0.2  # Adjust this fraction to reduce dataset size further if needed\n",
        "\n",
        "# Randomly sample the data\n",
        "df_sample = df.sample(frac=sample_fraction, random_state=42)\n",
        "print(\"Sampled subset shape:\", df_sample.shape)\n",
        "\n",
        "# Step 3: Save the Sampled Subset as a New CSV File\n",
        "output_path = 'small_cicids_sample.csv'\n",
        "df_sample.to_csv(output_path, index=False)\n",
        "print(f\"Sampled dataset saved as {output_path}\")"
      ],
      "metadata": {
        "id": "x0WV4911OUXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqJHTsgyBXkY"
      },
      "source": [
        "## Scaling the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e1ptt6mBXkY"
      },
      "source": [
        "The next few cells contain the code for scaling the data into the size adequate for the ML algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4xUBHHxBXkY"
      },
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "# ... (Your existing code for data cleaning and visualization) ...\n",
        "\n",
        "# --- Insert the data sampling code here ---\n",
        "import pandas as pd\n",
        "cleaned_dataset = pd.read_csv(\"/content/drive/MyDrive/Dataset_clean_dropna.csv\")\n",
        "sample_dataset = cleaned_dataset.sample(frac=0.2, random_state=42)\n",
        "\n",
        "# --- Replace the following line: ---\n",
        "# dataset = pd.read_csv(\"Dataset_clean_dropna.csv\")\n",
        "# --- with this line to use the sample: ---\n",
        "dataset = sample_dataset\n",
        "\n",
        "# ... (The rest of your existing code for data preparation, model training, etc.) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ge3HEnB1W122"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuWTSmK_BXkY"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3Qp3WKEBXkZ"
      },
      "outputs": [],
      "source": [
        "# For scaling the data, we use RobustScaler class from sklearn.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZLfARCnBXkZ"
      },
      "source": [
        "For scaling the data we used RobustScaler class from sklearn. RobustScaler is used to perserve outliers in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI1NF52kBXkZ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "scaler = RobustScaler()\n",
        "scaler.fit(features)\n",
        "\n",
        "features = scaler.transform(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPw_ryf-BXkZ"
      },
      "outputs": [],
      "source": [
        "# Checking if scaling has been succesful.\n",
        "features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O7jjpMsBXkZ"
      },
      "source": [
        "## Label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eON3IhLMBXkZ"
      },
      "source": [
        "Label encoding is done when dataset contains categorical values (ex. 0-5, A/B/C, 55+). It is used to turn categorical values into numerical values by replacing data categories with integers starting with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBJBSkXlBXkZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# No need to do previous operations, just load clean saved dataset.\n",
        "\n",
        "#dataset = pd.read_csv('Dataset_clean.csv', index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WkMz2g4BXkZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6kGbbgMBXkZ"
      },
      "source": [
        "'Lables' column contains categorical values - 15 of them (14 types of attacks in our dataset +  1 normal state).\n",
        "\n",
        "To convert this into numerical values we will use 'LabelEncoder' class from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JWSDQMwBXkZ"
      },
      "outputs": [],
      "source": [
        "LE = LabelEncoder()\n",
        "\n",
        "LE.fit(labels)\n",
        "labels = LE.transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCSOWf4YBXkZ"
      },
      "outputs": [],
      "source": [
        "# Labels have been replaced with integers.\n",
        "\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqDkmg39BXkZ"
      },
      "outputs": [],
      "source": [
        "# Checking that encoding reversal works.\n",
        "\n",
        "d = LE.inverse_transform(labels)\n",
        "d = pd.Series(d)\n",
        "d.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN38l9q9BXkZ"
      },
      "source": [
        "## Splitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBWmI16BXka"
      },
      "source": [
        "Final step to data preparation is splitting the data into traning and testing sets. For this there already exists _sklearn_ function that does all the splitting for us. This step is important so we can have representative data for evaluating our model. Both train and test samples should contain similar data variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN6cA2LRBXka"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG2DEm3cLAAE"
      },
      "outputs": [],
      "source": [
        "# The next step is to split training and testing data. For this we will use sklearn function train_test_split().\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=.2)\n",
        "\n",
        "features_train.shape, features_test.shape, labels_train.shape, labels_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjujpi95K9OF"
      },
      "outputs": [],
      "source": [
        "# Clearing variables.\n",
        "\n",
        "dataset = None\n",
        "finite = None\n",
        "labl = None\n",
        "d = None\n",
        "features = None\n",
        "labels = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kjCHB4524QE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Isolation Forest\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "print(\"Training Isolation Forest...\")\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "iso_forest.fit(features_train)\n",
        "iso_forest_pred = iso_forest.predict(features_test)\n",
        "# Convert predictions from {-1, 1} to {0, 1}\n",
        "iso_forest_pred = np.where(iso_forest_pred == -1, 1, 0)\n",
        "\n",
        "# 2. One-Class SVM\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "print(\"Training One-Class SVM...\")\n",
        "ocsvm = OneClassSVM(kernel='rbf', nu=0.1)\n",
        "ocsvm.fit(features_train)\n",
        "ocsvm_pred = ocsvm.predict(features_test)\n",
        "# Convert predictions\n",
        "ocsvm_pred = np.where(ocsvm_pred == -1, 1, 0)\n",
        "\n",
        "# 3. Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"Training Random Forest...\")\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(features_train, labels_train)\n",
        "rf_pred = rf_classifier.predict(features_test)\n",
        "\n",
        "# 4. Ensemble Approach\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "print(\"Training Ensemble...\")\n",
        "# Initialize base classifiers\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create and train ensemble\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('rf', rf), ('svm', svm), ('dt', dt)],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble.fit(features_train, labels_train)\n",
        "ensemble_pred = ensemble.predict(features_test)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "# Evaluate all models\n",
        "evaluate_model(labels_test, iso_forest_pred, \"Isolation Forest\")\n",
        "evaluate_model(labels_test, ocsvm_pred, \"One-Class SVM\")\n",
        "evaluate_model(labels_test, rf_pred, \"Random Forest\")\n",
        "evaluate_model(labels_test, ensemble_pred, \"Ensemble\")\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrix(labels_test, iso_forest_pred, 'Isolation Forest Confusion Matrix')\n",
        "plot_confusion_matrix(labels_test, ocsvm_pred, 'One-Class SVM Confusion Matrix')\n",
        "plot_confusion_matrix(labels_test, rf_pred, 'Random Forest Confusion Matrix')\n",
        "plot_confusion_matrix(labels_test, ensemble_pred, 'Ensemble Confusion Matrix')\n",
        "\n",
        "# ROC Curves\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc_curves(models_dict, X_test, y_test):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, model in models_dict.items():\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            # For models without predict_proba\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_proba = np.where(y_pred == -1, 1, 0)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Create dictionary of models for ROC curve comparison\n",
        "models_dict = {\n",
        "    'Random Forest': rf_classifier,\n",
        "    'Ensemble': ensemble\n",
        "}\n",
        "\n",
        "# Plot ROC curves (only for models with predict_proba)\n",
        "plot_roc_curves(models_dict, features_test, labels_test)\n",
        "\n",
        "# Save the best performing model\n",
        "import joblib\n",
        "\n",
        "# Assuming Random Forest performs best (you can change this based on results)\n",
        "best_model = rf_classifier\n",
        "joblib.dump(best_model, 'best_model.joblib')\n",
        "print(\"\\nBest model saved as 'best_model.joblib'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0_30YKQTw0m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import models\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 1. Isolation Forest\n",
        "print(\"Training Isolation Forest...\")\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "iso_forest.fit(features_train)\n",
        "iso_forest_pred = iso_forest.predict(features_test)\n",
        "# Convert predictions from {-1, 1} to {0, 1}\n",
        "iso_forest_pred = np.where(iso_forest_pred == -1, 1, 0)\n",
        "\n",
        "# 2. Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(features_train, labels_train)\n",
        "rf_pred = rf_classifier.predict(features_test)\n",
        "\n",
        "# 3. Decision Tree\n",
        "print(\"Training Decision Tree...\")\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(features_train, labels_train)\n",
        "dt_pred = dt_classifier.predict(features_test)\n",
        "\n",
        "# 4. K-Nearest Neighbors (KNN)\n",
        "print(\"Training K-Nearest Neighbors...\")\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(features_train, labels_train)\n",
        "knn_pred = knn_classifier.predict(features_test)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "# Evaluate all models\n",
        "evaluate_model(labels_test, iso_forest_pred, \"Isolation Forest\")\n",
        "evaluate_model(labels_test, rf_pred, \"Random Forest\")\n",
        "evaluate_model(labels_test, dt_pred, \"Decision Tree\")\n",
        "evaluate_model(labels_test, knn_pred, \"K-Nearest Neighbors\")\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrix(labels_test, iso_forest_pred, 'Isolation Forest Confusion Matrix')\n",
        "plot_confusion_matrix(labels_test, rf_pred, 'Random Forest Confusion Matrix')\n",
        "plot_confusion_matrix(labels_test, dt_pred, 'Decision Tree Confusion Matrix')\n",
        "plot_confusion_matrix(labels_test, knn_pred, 'K-Nearest Neighbors Confusion Matrix')\n",
        "\n",
        "# Save the best performing model\n",
        "import joblib\n",
        "\n",
        "# Assuming Random Forest performs best (you can change this based on results)\n",
        "best_model = rf_classifier\n",
        "joblib.dump(best_model, 'best_model.joblib')\n",
        "print(\"\\nBest model saved as 'best_model.joblib'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONyeeg5KatoK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import joblib\n",
        "\n",
        "# 1. Isolation Forest\n",
        "print(\"Training Isolation Forest...\")\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "iso_forest.fit(features_train)\n",
        "iso_forest_pred = iso_forest.predict(features_test)\n",
        "\n",
        "# Convert predictions from {-1, 1} to {0, 1} for evaluation\n",
        "iso_forest_pred = np.where(iso_forest_pred == -1, 1, 0)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    # Changed 'average' to 'weighted' for multiclass support\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "# Evaluate Isolation Forest model\n",
        "evaluate_model(labels_test, iso_forest_pred, \"Isolation Forest\")\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix for Isolation Forest\n",
        "plot_confusion_matrix(labels_test, iso_forest_pred, 'Isolation Forest Confusion Matrix')\n",
        "\n",
        "# Save the Isolation Forest model\n",
        "joblib.dump(iso_forest, 'isolation_forest_model.joblib')\n",
        "print(\"\\nIsolation Forest model saved as 'isolation_forest_model.joblib'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfG9kPbGcEcI"
      },
      "outputs": [],
      "source": [
        "# Summary of Anomaly Detection Results\n",
        "num_anomalies = np.sum(iso_forest_pred)\n",
        "num_normal = len(iso_forest_pred) - num_anomalies\n",
        "print(f\"\\nAnomaly Detection Summary:\")\n",
        "print(f\"Total data points: {len(iso_forest_pred)}\")\n",
        "print(f\"Detected anomalies: {num_anomalies}\")\n",
        "print(f\"Normal data points: {num_normal}\")\n",
        "\n",
        "# Show index and predictions for anomalies\n",
        "anomaly_indexes = np.where(iso_forest_pred == 1)[0]\n",
        "print(\"\\nIndexes of detected anomalies:\", anomaly_indexes)\n",
        "\n",
        "# Display the feature values of a few anomalies for inspection\n",
        "print(\"\\nExample of anomalous data points:\")\n",
        "# Use array indexing instead of .iloc\n",
        "print(features_test[anomaly_indexes[:5]])  # Shows the first 5 anomalies\n",
        "\n",
        "# Optional: Scatter plot (if you have two key features for visualization)\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Assuming features_test has two features for simplicity; otherwise, choose two features.\n",
        "# Use array indexing instead of .iloc\n",
        "plt.scatter(features_test[:, 0], features_test[:, 1], c=iso_forest_pred, cmap='coolwarm', label='Anomaly Detection')\n",
        "plt.title(\"Anomaly Detection Scatter Plot\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend([\"Normal\", \"Anomaly\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TdBPfRLde39"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming features_train and features_test are your training and testing features\n",
        "# Assuming labels_test is your ground truth labels for the test set\n",
        "\n",
        "print(\"Training Isolation Forest for Anomaly Detection...\")\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "iso_forest.fit(features_train)\n",
        "\n",
        "# Predicting anomalies\n",
        "iso_forest_pred = iso_forest.predict(features_test)\n",
        "# Convert predictions from {-1, 1} to {0, 1} where 1 represents anomalies\n",
        "iso_forest_pred = np.where(iso_forest_pred == -1, 1, 0)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    # Changed 'average' to 'weighted' for multiclass support\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=1):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "# Evaluate Isolation Forest\n",
        "evaluate_model(labels_test, iso_forest_pred, \"Isolation Forest\")\n",
        "\n",
        "# Summary of anomaly detection results\n",
        "total_data_points = len(features_test)\n",
        "detected_anomalies = np.sum(iso_forest_pred)\n",
        "normal_data_points = total_data_points - detected_anomalies\n",
        "\n",
        "print(\"\\nAnomaly Detection Summary:\")\n",
        "print(f\"Total test data points: {total_data_points}\")\n",
        "print(f\"Detected anomalies (potential intrusions): {detected_anomalies}\")\n",
        "print(f\"Normal data points: {normal_data_points}\")\n",
        "\n",
        "# Get indexes of detected anomalies for further inspection\n",
        "anomaly_indexes = np.where(iso_forest_pred == 1)[0]  # Indexes where anomalies are detected\n",
        "print(\"\\nIndexes of detected anomalies:\", anomaly_indexes)\n",
        "\n",
        "# Optional: Display some feature values for a few detected anomalies for inspection\n",
        "import pandas as pd\n",
        "features_test_df = pd.DataFrame(features_test)  # Convert to DataFrame for easier viewing\n",
        "anomalous_data = features_test_df.iloc[anomaly_indexes]\n",
        "print(\"\\nExample of anomalous data points:\")\n",
        "print(anomalous_data.head())  # Show details of the first few anomalies\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix for Isolation Forest\n",
        "plot_confusion_matrix(labels_test, iso_forest_pred, 'Isolation Forest Confusion Matrix')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yy6bgjqfNjk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "lr_model.fit(features_train, labels_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "lr_predictions = lr_model.predict(features_test)\n",
        "\n",
        "# Evaluate the model\n",
        "lr_accuracy = accuracy_score(labels_test, lr_predictions)\n",
        "lr_precision = precision_score(labels_test, lr_predictions, zero_division=1)\n",
        "lr_recall = recall_score(labels_test, lr_predictions)\n",
        "lr_f1 = f1_score(labels_test, lr_predictions)\n",
        "\n",
        "print(\"Logistic Regression Model Results:\")\n",
        "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Precision: {lr_precision:.4f}\")\n",
        "print(f\"Recall: {lr_recall:.4f}\")\n",
        "print(f\"F1-Score: {lr_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drE4kbeVBXka"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT0O-0DFBXka"
      },
      "source": [
        "For completing this task we chose to use a neural network. Specifically, the multi-layer perceptron, more specifically, feedforward neural network multi-class classifier with backpropagating algorithm. NN will be used to classifiy 14 different attacks and 1 normal state, as we saw from the labels in previous chapters.\n",
        "\n",
        "In this chapter we go by explaning parts of the network and its hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZoT_zFVBXka"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "#%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLt7rjCrBXka"
      },
      "source": [
        "Our tensorflow Sequential model has 3 layers. Input, 1 hidden and an output layer.\n",
        "\n",
        "* Input layer has 78 neurons, one for each feature.\n",
        "* Hidden layer has 67 neurons, this number has been calculated by [formula](https://www.heatonresearch.com/2017/06/01/hidden-layers.html) 2/3 the number of input neurons + number of output neurons.\n",
        "* Output layer has 15 neurons, one for each class we predict.\n",
        "\n",
        "For activation functions, we used standard functions for multi-class classification tasks - ReLu for hidden layer and _softmax_ function for output layer.\n",
        "\n",
        "Finally, we use Dropout parameter set to 0.2 for randomly shutting off 20% of neurons in each learning iteration. This technique is used for decreasing overfitting thereby incresing network accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Z6Oxi1BXka"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "    tf.keras.layers.Flatten(input_shape=(78,)),\n",
        "    tf.keras.layers.Dense(67, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(15, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpPY_qwIBXka"
      },
      "source": [
        "For learning rate optimization we used Adam optimizer.\n",
        "Loss function used is sparse categorical crossentropy, which is standard for multiclass classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf2kymtQBXka"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWYVUqXoBXka"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIXtqzsnBXka"
      },
      "source": [
        "In the next cell we setup training logs for tensorboard as well as some tensorboard callbacks.\n",
        "\n",
        "* tensorboard - callback that logs training data.\n",
        "* EarlyStopping - callback that monitors 'loss (function)' metric and if the loss function does not get better in tne hext 10 iterations, callback stops the training and resotres the network with best weights up untill that iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sweg40xJBXka"
      },
      "outputs": [],
      "source": [
        "log_dir = os.path.join(\n",
        "    \"train_logs\",\n",
        "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        ")\n",
        "\n",
        "# TF callback that sets up TensorBoard with training logs.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# TF callback that stops training when best value of validationi loss function is reached. It also\n",
        "# restores weights from the best training iteration.\n",
        "eary_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4S6KJXdBXkb"
      },
      "outputs": [],
      "source": [
        "model.fit(features_train,\n",
        "          labels_train,\n",
        "          epochs=100,\n",
        "          callbacks=[tensorboard_callback, eary_stop_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHjVHWKeBXkb"
      },
      "source": [
        "We can see that training stoped after 18 out of 100 epochs due to 'loss' function metric not changing much in the previous 10 epochs.\n",
        "\n",
        "After training we evaluate model accuracy (next cell), and find that our model predicts attacks with **91.2% accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-nhvfOiBXkb"
      },
      "outputs": [],
      "source": [
        "# Evaluating model accuracy.\n",
        "model.evaluate(features_test, labels_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihk5nwTuBXkb"
      },
      "outputs": [],
      "source": [
        "# Saving the model.\n",
        "\n",
        "model.save('saved_models/IDS_model_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izMEGrNfBXkb"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsZMDg1aBXkb"
      },
      "source": [
        "In this project we made a neural network classifier that can predict 14 network/web attacks and normal traffic with 91% accuracy. This model is proof-of-concept that feedforward neural network with backpropagation algorithm can be used for classifying attacks in anomaly-based intrusion detection systems.\n",
        "\n",
        "\n",
        "**Propositions**\n",
        "\n",
        "We propose a couple of solutions for improving model accuracy as well as use of some other neural network architectures.\n",
        "\n",
        "Accuracy of this model can probably be improved by _feature engineering_ and _feature selection_. Picking the features that have the most influence on the model.\n",
        "\n",
        "Regarding this model, we propose tuning the model hyperparameters. Changing the hidden layer activation function, early stopping callback, dropout, optimizer and loss function should increase accuracy by some extent. Another way, albeit more complicated and resource intense is to use a genetic algorithm to evolve the best neural network arhitecture for this specific task.\n",
        "\n",
        "Finally, we propose the usage of some other ML algorithms. Random forest classifiers have been used in intrusion detection system for a while now. Alternatively, we found some sources using autoencoders for anomaly detection."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "375.994px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "485.188px",
        "left": "645.125px",
        "right": "20px",
        "top": "174.938px",
        "width": "628.75px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}